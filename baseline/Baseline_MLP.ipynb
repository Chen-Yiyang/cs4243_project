{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e688d616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyang/miniconda3/envs/cv_proj/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7289a3",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e625eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../'\n",
    "full_data = torch.load(data_path + 'unaug_data_small.pt').float().contiguous()\n",
    "full_label = torch.load(data_path + 'unaug_label.pt')\n",
    "full_label = torch.tensor(full_label)\n",
    "\n",
    "full_label = full_label - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134569b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split into training and testing 80%-20%\n",
    "train_data = full_data\n",
    "train_label = full_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d986f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data,  torch.Size([6112, 3, 32, 32])\n",
      "Training label, torch.Size([6112])\n"
     ]
    }
   ],
   "source": [
    "# show size\n",
    "print(\"Training data, \", train_data.size())\n",
    "print(\"Training label,\", train_label.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d107db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6112\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train_data_size = train_data.size(0)\n",
    "print(train_data_size)\n",
    "print(train_data.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f34455",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "- display num param\n",
    "- get error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util func\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('There are {} ({:.2f} million) parameters in this neural network'.format(\n",
    "        nb_param, nb_param/1e6)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f2683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error( scores , labels ):\n",
    "    bs=scores.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_matches=indicator.sum()\n",
    "    \n",
    "    return 1-num_matches.float()/bs   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43011e4d",
   "metadata": {},
   "source": [
    "## Build MLP\n",
    "A 3-layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9daccc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size):\n",
    "        super(three_layer_net , self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = torch.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71be6112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_layer_net(\n",
      "  (layer1): Linear(in_features=3072, out_features=100, bias=True)\n",
      "  (layer2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (layer3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "There are 318410 (0.32 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "net=three_layer_net(3072,100,100,10)\n",
    "print(net)\n",
    "display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3ced9",
   "metadata": {},
   "source": [
    "## Define Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a2a71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD( net.parameters() , lr=0.01 )\n",
    "bs= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec7238",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3163c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1965, 3309, 5946,  ...,  968, 5295, 4519])\n",
      "epoch= 0 \t time= 0.2573399543762207 \t loss= 2.237499209701038 \t error= 83.6393446218772 percent\n",
      " \n",
      "tensor([4777, 3972, 4668,  ..., 5290, 1246, 2796])\n",
      "epoch= 1 \t time= 0.45740509033203125 \t loss= 2.2345253170513715 \t error= 83.68852500055657 percent\n",
      " \n",
      "tensor([4971, 4917,  901,  ..., 1197, 2419,  357])\n",
      "epoch= 2 \t time= 0.6969962120056152 \t loss= 2.2314928672352776 \t error= 83.6393449150148 percent\n",
      " \n",
      "tensor([  45, 3488, 4823,  ..., 5236, 2808, 2006])\n",
      "epoch= 3 \t time= 0.8927221298217773 \t loss= 2.2288295831836638 \t error= 83.62295099946319 percent\n",
      " \n",
      "tensor([2538, 1432, 2045,  ..., 4496, 4940, 4205])\n",
      "epoch= 4 \t time= 1.1034281253814697 \t loss= 2.2266235546987563 \t error= 83.60655776789932 percent\n",
      " \n",
      "tensor([5227,   94, 5983,  ..., 3024,  601, 1852])\n",
      "epoch= 5 \t time= 1.288559913635254 \t loss= 2.2248845295827895 \t error= 83.65573795115363 percent\n",
      " \n",
      "tensor([ 894, 2631, 3232,  ...,  570, 1389, 3924])\n",
      "epoch= 6 \t time= 1.5014359951019287 \t loss= 2.223243643025883 \t error= 83.62295139031332 percent\n",
      " \n",
      "tensor([1058, 4760, 4593,  ..., 3794, 1273, 2307])\n",
      "epoch= 7 \t time= 1.6898441314697266 \t loss= 2.221414487870013 \t error= 83.62295158573838 percent\n",
      " \n",
      "tensor([4387, 4772, 5829,  ..., 5668, 3426, 4629])\n",
      "epoch= 8 \t time= 1.8979530334472656 \t loss= 2.220464413283301 \t error= 83.63934442645214 percent\n",
      " \n",
      "tensor([ 777, 1518, 3315,  ..., 4407, 5001, 2739])\n",
      "epoch= 9 \t time= 2.087007999420166 \t loss= 2.2196044921875 \t error= 83.65573834200374 percent\n",
      " \n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    shuffled_indices=torch.randperm(train_data_size)\n",
    "    print(shuffled_indices)\n",
    " \n",
    "    for count in range(0, (train_data_size//bs)*bs ,bs):\n",
    "    \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_data[indices]\n",
    "        minibatch_label=  train_label[indices]\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(bs,3072)  # to continuous tensor\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores=net( inputs ) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss =  criterion( scores , minibatch_label) \n",
    "        \n",
    "        # backward pass to compute dL/dU, dL/dV and dL/dW   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1        \n",
    "    \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        #eval_on_test_set() \n",
    "        print(' ')\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00ef4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
